{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## [Tensor](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
    "\n",
    "Tensor就是张量，一种容纳数据的多维容器，和多维数组类似，主要用于存储用于训练等方面的数据。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## [Tensor构造函数](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor)\n",
    "```python\n",
    "torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor\n",
    "```\n",
    "Constructs a tensor with no autograd history (also known as a “leaf tensor”) by copying data.\n",
    "\n",
    "通过复制数据的方式构建一个张量。\n",
    "\n",
    "**Parameters**\n",
    "* **data**(array_like) - 为tensor初始化的数据，可以是list, tuple, NumPy ndarry, scalar等类型。\n",
    "* **dtype**            - 可选参数，为[torch.dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)中的类型，tensor的数据类型，如果为None则以参数data中的数据类型为准。\n",
    "* **device**           - 可选参数，为[torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)中的类型，tensor的运行设备，一般为\"cpu\"或者\"cuda\",如果为None时以参数data中指定的数据为准，若data无指定则为当前设备。\n",
    "* **requires_grad**    - 可选参数，bool类型，为True时告诉这个tensor需要进行求导求梯度grad操作。\n",
    "* **pin_memory**       - 可选参数，bool类型，为True时说明tensor会放入固定内存中，只对cpu tensor生效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "零阶张量\n",
      "tensor(123)\n",
      "-----------------------\n",
      "一阶张量\n",
      "tensor([123, 234, 567])\n",
      "tensor(234)\n",
      "-----------------------\n",
      "二阶张量\n",
      "tensor([[123, 234, 345],\n",
      "        [456, 567, 678],\n",
      "        [789, 890, 901]])\n",
      "tensor([456, 567, 678])\n",
      "tensor(678)\n",
      "-----------------------\n",
      "指定类型和设备\n",
      "tensor([[0.1111, 0.2222, 0.3333]], device='cuda:0', dtype=torch.float64)\n",
      "-----------------------\n",
      "numpy\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "-----------------------\n",
      "初始化全0tensor\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n",
      "-----------------------\n",
      "初始化全1tensor\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n",
      "-----------------------\n",
      "随机tensor\n",
      "tensor([[0.1463, 0.0087, 0.6814, 0.6625],\n",
      "        [0.6310, 0.2867, 0.3957, 0.8345],\n",
      "        [0.9719, 0.0852, 0.8702, 0.8879]])\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 零阶张量 -> 常量\n",
    "tensor0d = torch.tensor(123)\n",
    "print(\"零阶张量\")\n",
    "print(tensor0d)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 一阶张量 -> 向量 -> 一维数组\n",
    "tensor1d = torch.tensor([123, 234, 567])\n",
    "print(\"一阶张量\")\n",
    "print(tensor1d)\n",
    "# tensor[idx] 类似数组的读数据方法\n",
    "print(tensor1d[1])\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 二阶张量 -> 矩阵 -> 二维数组\n",
    "tensor2d = torch.tensor([[123, 234, 345],\n",
    "                         [456, 567, 678],\n",
    "                         [789, 890, 901]])\n",
    "print(\"二阶张量\")\n",
    "print(tensor2d)\n",
    "print(tensor2d[1])\n",
    "print(tensor2d[1][2])\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 指定dtype和device\n",
    "print(\"指定类型和设备\")\n",
    "tensor1 = torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                        dtype=torch.float64,\n",
    "                        device=torch.device('cuda:0'))\n",
    "print(tensor1)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# np.array\n",
    "tensor2 = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "print(\"numpy\")\n",
    "print(tensor2)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 初始化全0tensor\n",
    "tensor_all0 = torch.zeros([2,4])\n",
    "print(\"初始化全0tensor\")\n",
    "print(tensor_all0)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 初始化全1tensor\n",
    "tensor_all1 = torch.ones([2,4], dtype=torch.int32)\n",
    "print(\"初始化全1tensor\")\n",
    "print(tensor_all1)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 随机tensor\n",
    "tensor_r = torch.rand(3,4)\n",
    "print(\"随机tensor\")\n",
    "print(tensor_r)\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## [Tensor属性](https://pytorch.org/docs/stable/tensor_attributes.html)\n",
    "每个tensor都有torch.dtype, torch.device, torch.layout三个属性。\n",
    "### torch.dtype\n",
    "dtype为torch提供的数据格式。\n",
    "| Data Type | dtype |\n",
    "| --- | --- |\n",
    "| 32-bit floating point | torch.float32 or torch.float |\n",
    "| 64-bit floating point | torch.float64 or torch.double |\n",
    "| 64-bit complex | torch.complex64 or torch.cfloat |\n",
    "| 128-bit complex | torch.complex128 or torch.cdouble |\n",
    "| 16-bit floating point | torch.float16 or torch.half |\n",
    "| 16-bit floating point | torch.bfloat16 |\n",
    "| 8-bit integer (unsigned) | torch.uint8 |\n",
    "| 8-bit integer (signed) | torch.int8 |\n",
    "| 16-bit integer (signed) | torch.int16 or torch.short |\n",
    "| 32-bit integer (signed) | torch.int32 or torch.int |\n",
    "| 64-bit integer (signed) | torch.int64 or torch.long |\n",
    "| Boolean | torch.bool |\n",
    "### torch.device\n",
    "device为一个tensor所属或者将要部署的设备。\n",
    "\n",
    "device包括一个设备类型（通常是\"cpu\"或者\"cuda\"，还可以是\"mps\",\"xpu\",\"xia\",\"meta\"）和设备的序号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda:0') #序号为0的cuda gpu设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cpu') #cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda') #当前cuda设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.layout\n",
    "layout表示tensor的内存布局，目前pytorch支持torch.strided和torch.sparse_coo即顺序存储和离散存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## tensor基础运算\n",
    "### 1. 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6,  8],\n",
      "        [ 6,  8, 10]])\n",
      "tensor([[ 4,  6,  8],\n",
      "        [ 6,  8, 10]])\n",
      "tensor([[ 4,  6,  8],\n",
      "        [ 6,  8, 10]])\n",
      "tensor([[ 6,  8, 10],\n",
      "        [ 7,  9, 11]])\n",
      "tensor([[ 8, 10, 12],\n",
      "        [ 9, 11, 13]])\n",
      "tensor([[ 8, 10, 12],\n",
      "        [ 9, 11, 13]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 4, 5], [4, 5, 6]])\n",
    "c = torch.tensor([5, 6, 7])\n",
    "\n",
    "d1 = a + b\n",
    "d2 = torch.add(a, b)\n",
    "d3 = a.add(b)\n",
    "# 这几个Tensor加减乘除会对c自动进行Broadcasting\n",
    "d4 = a + c\n",
    "d5 = torch.add(b, c)\n",
    "d6 = c.add(b)\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(d3)\n",
    "print(d4)\n",
    "print(d5)\n",
    "print(d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 减法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  2,  4],\n",
      "        [-2, -2, -2]])\n",
      "tensor([[ 0,  2,  4],\n",
      "        [-2, -2, -2]])\n",
      "tensor([[ 0,  2,  4],\n",
      "        [-2, -2, -2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[3, 4, 5], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 2, 1], [4, 5, 6]])\n",
    "\n",
    "c1 = a - b\n",
    "c2 = torch.sub(a, b)\n",
    "c3 = a.sub(b)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  8, 15],\n",
      "        [ 8, 15, 24]])\n",
      "tensor([[ 3,  8, 15],\n",
      "        [ 8, 15, 24]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 4, 5], [4, 5, 6]])\n",
    "\n",
    "c1 = a * b\n",
    "c2 = torch.mul(a, b)\n",
    "c3 = a.mul(b)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.6000, 0.6667]])\n",
      "tensor([[2.0000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.6000, 0.6667]])\n",
      "tensor([[2.0000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.6000, 0.6667]])\n",
      "dtype of a  torch.int64\n",
      "dtype of c1 torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[6, 2, 3], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 4, 5], [4, 5, 6]])\n",
    "\n",
    "c1 = a / b\n",
    "c2 = torch.div(a, b)\n",
    "c3 = a.div(b)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)\n",
    "print(\"dtype of a  \" + str(a.dtype))\n",
    "print(\"dtype of c1 \" + str(c1.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26)\n",
      "tensor(26)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([3, 4, 5])\n",
    "\n",
    "c1 = torch.dot(a, b)\n",
    "c2 = a.dot(b)\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32]])\n",
      "tensor([[4, 8]])\n",
      "tensor([[11]])\n",
      "tensor([[ 7, 10]])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# 2维矩阵\n",
    "d1_1 = torch.tensor([[4]])\n",
    "d1_2 = torch.tensor([[8]])\n",
    "d1x2 = torch.tensor([[1, 2]])\n",
    "d2x1 = torch.tensor([[3], [4]])\n",
    "d2_1 = torch.tensor([[1, 2], [3, 4]])\n",
    "d2_2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "r1 = d1_1 @ d1_2                # dot product\n",
    "r2 = torch.mm(d1_1, d1x2)       # 1x1 and 1x2\n",
    "# r3 = torch.matmul(d1_1, d2x1) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x1 and 2x1)\n",
    "# r3 = torch.matmul(d1x2, d1x2) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x2 and 1x2)\n",
    "# r3 = torch.matmul(d2x1, d2x1) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x1)\n",
    "r4 = d1x2 @ d2x1                # 1x2 and 2x1\n",
    "r5 = d1x2.mm(d2_1)              # 1x2 and 2x2\n",
    "# r6 = d2x1.matmul(d2_1)        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x2)\n",
    "r7 = d2_1 @ d2_2                # 2x2 and 2x2\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r4)\n",
    "print(r5)\n",
    "print(r7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "b = a.pow(2)\n",
    "c = a ** 2\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 开方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1.0000, 0.5000],\n",
      "        [0.3333, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = a.pow(2)\n",
    "\n",
    "c = b.sqrt()\n",
    "d = b ** (0.5)\n",
    "e = b.rsqrt()\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 指数和对数"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
