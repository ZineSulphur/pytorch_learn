{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## [Tensor](https://pytorch.org/docs/stable/tensors.html)\n",
    "\n",
    "A torch.Tensor is a multi-dimensional matrix containing elements of a single data type.\n",
    "\n",
    "Tensor就是张量，一种容纳数据的多维容器，和多维数组类似，主要用于存储用于训练等方面的数据。\n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 创建tensor\n",
    "### [Tensor构造函数](https://pytorch.org/docs/stable/generated/torch.tensor.html#torch.tensor)\n",
    "```python\n",
    "torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False) → Tensor\n",
    "```\n",
    "Constructs a tensor with no autograd history (also known as a “leaf tensor”) by copying data.\n",
    "\n",
    "通过复制数据的方式构建一个张量。\n",
    "\n",
    "**Parameters**\n",
    "* **data**(array_like) - 为tensor初始化的数据，可以是list, tuple, NumPy ndarry, scalar等类型。\n",
    "* **dtype**            - 可选参数，为[torch.dtype](https://pytorch.org/docs/stable/tensor_attributes.html#torch.dtype)中的类型，tensor的数据类型，如果为None则以参数data中的数据类型为准。\n",
    "* **device**           - 可选参数，为[torch.device](https://pytorch.org/docs/stable/tensor_attributes.html#torch.device)中的类型，tensor的运行设备，一般为\"cpu\"或者\"cuda\",如果为None时以参数data中指定的数据为准，若data无指定则为当前设备。\n",
    "* **requires_grad**    - 可选参数，bool类型，为True时告诉这个tensor需要进行求导求梯度grad操作。\n",
    "* **pin_memory**       - 可选参数，bool类型，为True时说明tensor会放入固定内存中，只对cpu tensor生效。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "零阶张量\n",
      "tensor(123)\n",
      "-----------------------\n",
      "一阶张量\n",
      "tensor([123, 234, 567])\n",
      "tensor(234)\n",
      "-----------------------\n",
      "二阶张量\n",
      "tensor([[123, 234, 345],\n",
      "        [456, 567, 678],\n",
      "        [789, 890, 901]])\n",
      "tensor([456, 567, 678])\n",
      "tensor(678)\n",
      "-----------------------\n",
      "指定类型和设备\n",
      "tensor([[0.1111, 0.2222, 0.3333]], device='cuda:0', dtype=torch.float64)\n",
      "-----------------------\n",
      "numpy\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]], dtype=torch.int32)\n",
      "-----------------------\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "# 零阶张量 -> 常量\n",
    "tensor0d = torch.tensor(123)\n",
    "print(\"零阶张量\")\n",
    "print(tensor0d)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 一阶张量 -> 向量 -> 一维数组\n",
    "tensor1d = torch.tensor([123, 234, 567])\n",
    "print(\"一阶张量\")\n",
    "print(tensor1d)\n",
    "# tensor[idx] 类似数组的读数据方法\n",
    "print(tensor1d[1])\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 二阶张量 -> 矩阵 -> 二维数组\n",
    "tensor2d = torch.tensor([[123, 234, 345],\n",
    "                         [456, 567, 678],\n",
    "                         [789, 890, 901]])\n",
    "print(\"二阶张量\")\n",
    "print(tensor2d)\n",
    "print(tensor2d[1])\n",
    "print(tensor2d[1][2])\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# 指定dtype和device\n",
    "print(\"指定类型和设备\")\n",
    "tensor1 = torch.tensor([[0.11111, 0.222222, 0.3333333]],\n",
    "                        dtype=torch.float64,\n",
    "                        device=torch.device('cuda:0'))\n",
    "print(tensor1)\n",
    "print(\"-----------------------\")\n",
    "\n",
    "# np.array\n",
    "tensor2 = torch.tensor(np.array([[1, 2, 3], [4, 5, 6]]))\n",
    "print(\"numpy\")\n",
    "print(tensor2)\n",
    "print(\"-----------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 通过数值创建\n",
    "#### 1. [创建全0张量](https://pytorch.org/docs/stable/generated/torch.zeros.html)\n",
    "```python\n",
    "torch.zeros(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "```\n",
    "**parameters**\n",
    "* size      - 张量的形状，可以是变长参数或者list或者tuple\n",
    "* out       - 输出的张量，指定out时返回的张量和out共享一个内存\n",
    "* dtype     - 张量的数据类型\n",
    "* layout    - 内存方式\n",
    "* device    - 张量所在设备\n",
    "* requires_grad - 是否需要梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化全0tensor\n",
      "tensor([[0., 0., 0., 0.],\n",
      "        [0., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "# 初始化全0tensor\n",
    "tensor_all0 = torch.zeros([2,4])\n",
    "print(\"初始化全0tensor\")\n",
    "print(tensor_all0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. [创建全1张量](https://pytorch.org/docs/stable/generated/torch.ones.html#torch.ones)\n",
    "```python\n",
    "torch.ones(*size, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "```\n",
    "参数和torch.zeros相同"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化全1tensor\n",
      "tensor([[1, 1, 1, 1],\n",
      "        [1, 1, 1, 1]], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "# 初始化全1tensor\n",
    "tensor_all1 = torch.ones([2,4], dtype=torch.int32)\n",
    "print(\"初始化全1tensor\")\n",
    "print(tensor_all1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. [创建自定义张量](https://pytorch.org/docs/stable/generated/torch.full.html)\n",
    "```python\n",
    "torch.full(size, fill_value, *, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False) → Tensor\n",
    "```\n",
    "**parameters**\n",
    "* size      - 张量的形状，可以是变长参数或者list或者tuple\n",
    "* fill_value    - 填充的值\n",
    "* out       - 输出的张量，指定out时返回的张量和out共享一个内存\n",
    "* dtype     - 张量的数据类型\n",
    "* layout    - 内存方式\n",
    "* device    - 张量所在设备\n",
    "* requires_grad - 是否需要梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "初始化自定义tensor\n",
      "tensor([[3.1416, 3.1416, 3.1416],\n",
      "        [3.1416, 3.1416, 3.1416]])\n"
     ]
    }
   ],
   "source": [
    "tensor_sd = torch.full((2, 3), 3.141592)\n",
    "print(\"初始化自定义tensor\")\n",
    "print(tensor_sd)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. [创建随机张量](https://pytorch.org/docs/stable/generated/torch.rand.html)\n",
    "```python\n",
    "torch.rand(*size, *, generator=None, out=None, dtype=None, layout=torch.strided, device=None, requires_grad=False, pin_memory=False) → Tensor\n",
    "```\n",
    "**parameters**\n",
    "* size      - 张量的形状，可以是变长参数或者list或者tuple\n",
    "* generator - 随机数生成器，为torch.Generator中的类型\n",
    "* out       - 输出的张量，指定out时返回的张量和out共享一个内存\n",
    "* dtype     - 张量的数据类型\n",
    "* layout    - 内存方式\n",
    "* device    - 张量所在设备\n",
    "* requires_grad - 是否需要梯度\n",
    "* pin_memory    - 是否放在固定内存中"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "随机tensor\n",
      "tensor([[0.1843, 0.6749, 0.3451, 0.1405],\n",
      "        [0.7515, 0.9359, 0.2426, 0.4955],\n",
      "        [0.5607, 0.5223, 0.1263, 0.5433]])\n"
     ]
    }
   ],
   "source": [
    "# 随机tensor\n",
    "tensor_r = torch.rand(3,4)\n",
    "print(\"随机tensor\")\n",
    "print(tensor_r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 模仿别的张量大小创建\n",
    "```python\n",
    "torch.zeros_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "torch.ones_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "torch.full_like(input, fill_value, *, dtype=None, layout=torch.strided, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "\n",
    "torch.rand_like(input, *, dtype=None, layout=None, device=None, requires_grad=False, memory_format=torch.preserve_format) → Tensor\n",
    "```\n",
    "**parameters**\n",
    "* input  - 输入的张量的大小决定输出的张量的大小\n",
    "* memory_format - 返回的张量所需的内存格式"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0, 0, 0],\n",
      "        [0, 0, 0]])\n",
      "tensor([[1, 1, 1],\n",
      "        [1, 1, 1]])\n",
      "tensor([[10, 10, 10],\n",
      "        [10, 10, 10]])\n",
      "tensor([[0.6366, 0.8109, 0.3305],\n",
      "        [0.6182, 0.3752, 0.3884]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "b = torch.tensor([[1, 2, 3], [4, 5, 6]], dtype=torch.float32)\n",
    "\n",
    "t_all0 = torch.zeros_like(a)\n",
    "t_all1 = torch.ones_like(a)\n",
    "t_full = torch.full_like(a, 10)\n",
    "t_rand = torch.rand_like(b) # torch.rand_like输入需为浮点型数值\n",
    "print(t_all0)\n",
    "print(t_all1)\n",
    "print(t_full)\n",
    "print(t_rand)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. 其他创建方式\n",
    "* torch.arange() 等差数列一阶张量\n",
    "* torch.linspace() 均分一阶张量\n",
    "* torch.logspace() 对数均分一阶张量\n",
    "* torch.eye() 对角二维矩阵\n",
    "* torch.normal() 正态分布"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([2, 4, 6, 8])\n",
      "tensor([ 2.,  6., 10.])\n",
      "tensor([  100.,  1000., 10000.])\n",
      "tensor([[1., 0., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 0., 1.]])\n",
      "tensor([[1., 0., 0., 0.],\n",
      "        [0., 1., 0., 0.],\n",
      "        [0., 0., 1., 0.]])\n",
      "tensor([ 0.0969, -1.9230,  1.6957,  0.0586])\n"
     ]
    }
   ],
   "source": [
    "a = torch.arange(2, 10, 2)\n",
    "b = torch.linspace(2, 10, 3)\n",
    "c = torch.logspace(2, 4, 3)\n",
    "d = torch.eye(3)\n",
    "e = torch.eye(3,4)\n",
    "f = torch.normal(0., 1., size=(4,))\n",
    "print(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## [Tensor属性](https://pytorch.org/docs/stable/tensor_attributes.html)\n",
    "每个tensor都有torch.dtype, torch.device, torch.layout属性。\n",
    "### torch.dtype\n",
    "dtype为torch提供的数据格式。\n",
    "| Data Type | dtype |\n",
    "| --- | --- |\n",
    "| 32-bit floating point | torch.float32 or torch.float |\n",
    "| 64-bit floating point | torch.float64 or torch.double |\n",
    "| 64-bit complex | torch.complex64 or torch.cfloat |\n",
    "| 128-bit complex | torch.complex128 or torch.cdouble |\n",
    "| 16-bit floating point | torch.float16 or torch.half |\n",
    "| 16-bit floating point | torch.bfloat16 |\n",
    "| 8-bit integer (unsigned) | torch.uint8 |\n",
    "| 8-bit integer (signed) | torch.int8 |\n",
    "| 16-bit integer (signed) | torch.int16 or torch.short |\n",
    "| 32-bit integer (signed) | torch.int32 or torch.int |\n",
    "| 64-bit integer (signed) | torch.int64 or torch.long |\n",
    "| Boolean | torch.bool |\n",
    "### torch.device\n",
    "device为一个tensor所属或者将要部署的设备。\n",
    "\n",
    "device包括一个设备类型（通常是\"cpu\"或者\"cuda\"，还可以是\"mps\",\"xpu\",\"xia\",\"meta\"）和设备的序号。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda:0') #序号为0的cuda gpu设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cpu') #cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda') #当前cuda设备"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda', index=0)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.device('cuda', 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### torch.layout\n",
    "layout表示tensor的内存布局，目前pytorch支持torch.strided和torch.sparse_coo即顺序存储和离散存储"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tensor.shape\n",
    "张量形状\n",
    "### Tensor.ndim\n",
    "张量维度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 3])\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6]])\n",
    "print(a.shape)\n",
    "print(a.ndim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## tensor基础运算\n",
    "### 1. 加法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 4,  6,  8],\n",
      "        [ 6,  8, 10]])\n",
      "tensor([[ 4,  6,  8],\n",
      "        [ 6,  8, 10]])\n",
      "tensor([[ 4,  6,  8],\n",
      "        [ 6,  8, 10]])\n",
      "tensor([[ 6,  8, 10],\n",
      "        [ 7,  9, 11]])\n",
      "tensor([[ 8, 10, 12],\n",
      "        [ 9, 11, 13]])\n",
      "tensor([[ 8, 10, 12],\n",
      "        [ 9, 11, 13]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "a = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 4, 5], [4, 5, 6]])\n",
    "c = torch.tensor([5, 6, 7])\n",
    "\n",
    "d1 = a + b\n",
    "d2 = torch.add(a, b)\n",
    "d3 = a.add(b)\n",
    "# 这几个Tensor加减乘除会对c自动进行Broadcasting\n",
    "d4 = a + c\n",
    "d5 = torch.add(b, c)\n",
    "d6 = c.add(b)\n",
    "print(d1)\n",
    "print(d2)\n",
    "print(d3)\n",
    "print(d4)\n",
    "print(d5)\n",
    "print(d6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 减法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 0,  2,  4],\n",
      "        [-2, -2, -2]])\n",
      "tensor([[ 0,  2,  4],\n",
      "        [-2, -2, -2]])\n",
      "tensor([[ 0,  2,  4],\n",
      "        [-2, -2, -2]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[3, 4, 5], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 2, 1], [4, 5, 6]])\n",
    "\n",
    "c1 = a - b\n",
    "c2 = torch.sub(a, b)\n",
    "c3 = a.sub(b)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. 乘法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  8, 15],\n",
      "        [ 8, 15, 24]])\n",
      "tensor([[ 3,  8, 15],\n",
      "        [ 8, 15, 24]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 4, 5], [4, 5, 6]])\n",
    "\n",
    "c1 = a * b\n",
    "c2 = torch.mul(a, b)\n",
    "c3 = a.mul(b)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. 除法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[2.0000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.6000, 0.6667]])\n",
      "tensor([[2.0000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.6000, 0.6667]])\n",
      "tensor([[2.0000, 0.5000, 0.6000],\n",
      "        [0.5000, 0.6000, 0.6667]])\n",
      "dtype of a  torch.int64\n",
      "dtype of c1 torch.float32\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[6, 2, 3], [2, 3, 4]])\n",
    "b = torch.tensor([[3, 4, 5], [4, 5, 6]])\n",
    "\n",
    "c1 = a / b\n",
    "c2 = torch.div(a, b)\n",
    "c3 = a.div(b)\n",
    "print(c1)\n",
    "print(c2)\n",
    "print(c3)\n",
    "print(\"dtype of a  \" + str(a.dtype))\n",
    "print(\"dtype of c1 \" + str(c1.dtype))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. 点积"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(26)\n",
      "tensor(26)\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([1, 2, 3])\n",
    "b = torch.tensor([3, 4, 5])\n",
    "\n",
    "c1 = torch.dot(a, b)\n",
    "c2 = a.dot(b)\n",
    "print(c1)\n",
    "print(c2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. 矩阵相乘"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[32]])\n",
      "tensor([[4, 8]])\n",
      "tensor([[11]])\n",
      "tensor([[ 7, 10]])\n",
      "tensor([[19, 22],\n",
      "        [43, 50]])\n"
     ]
    }
   ],
   "source": [
    "# 2维矩阵\n",
    "d1_1 = torch.tensor([[4]])\n",
    "d1_2 = torch.tensor([[8]])\n",
    "d1x2 = torch.tensor([[1, 2]])\n",
    "d2x1 = torch.tensor([[3], [4]])\n",
    "d2_1 = torch.tensor([[1, 2], [3, 4]])\n",
    "d2_2 = torch.tensor([[5, 6], [7, 8]])\n",
    "\n",
    "r1 = d1_1 @ d1_2                # dot product\n",
    "r2 = torch.mm(d1_1, d1x2)       # 1x1 and 1x2\n",
    "# r3 = torch.matmul(d1_1, d2x1) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x1 and 2x1)\n",
    "# r3 = torch.matmul(d1x2, d1x2) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (1x2 and 1x2)\n",
    "# r3 = torch.matmul(d2x1, d2x1) # RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x1)\n",
    "r4 = d1x2 @ d2x1                # 1x2 and 2x1\n",
    "r5 = d1x2.mm(d2_1)              # 1x2 and 2x2\n",
    "# r6 = d2x1.matmul(d2_1)        # RuntimeError: mat1 and mat2 shapes cannot be multiplied (2x1 and 2x2)\n",
    "r7 = d2_1 @ d2_2                # 2x2 and 2x2\n",
    "print(r1)\n",
    "print(r2)\n",
    "print(r4)\n",
    "print(r5)\n",
    "print(r7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. 幂运算"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n",
      "tensor([[ 1,  4],\n",
      "        [ 9, 16]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "\n",
    "b = a.pow(2)\n",
    "c = a ** 2\n",
    "print(b)\n",
    "print(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. 开方"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1.0000, 0.5000],\n",
      "        [0.3333, 0.2500]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "b = a.pow(2)\n",
    "\n",
    "c = b.sqrt()\n",
    "d = b ** (0.5)\n",
    "e = b.rsqrt()\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. 指数和对数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 2.7183,  7.3891],\n",
      "        [20.0855, 54.5981]])\n",
      "tensor([[ 2.,  4.],\n",
      "        [ 8., 16.]])\n",
      "tensor([[ 1.7183,  6.3891],\n",
      "        [19.0855, 53.5981]])\n",
      "-------------------\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[1., 2.],\n",
      "        [3., 4.]])\n",
      "tensor([[0., 1.],\n",
      "        [2., 3.]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2], [3, 4]])\n",
    "a1 = torch.tensor([[1, 10], [100, 1000]])\n",
    "\n",
    "b = torch.exp(a)\n",
    "c = torch.exp2(a)\n",
    "d = torch.expm1(a)\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(\"-------------------\")\n",
    "\n",
    "e = torch.log(b)\n",
    "f = torch.log2(c)\n",
    "g = torch.log10(a1)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "## tensor 索引和切片\n",
    "### tensor访问\n",
    "使用类似Numpy的方式访问tensor\n",
    "\n",
    "* tensor[index]             - 降维访问第index个数据，类似于数组操作\n",
    "* tensor[start:end]         - 切片访问[start,end)范围内的数据\n",
    "* tensor[start:]            - 切片访问，省略end表示访问到最后一个元素结束\n",
    "* tensor[:end]              - 切片访问，省略start表示访问从第一个元素开始\n",
    "* tensor[:]                 - 访问所有元素\n",
    "* tensor[index,start:end]   - 降维访问第index个数据,并进行切片"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3])\n",
      "tensor(2)\n",
      "tensor([[4, 5, 6]])\n",
      "tensor([[ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "tensor([[1, 2, 3],\n",
      "        [4, 5, 6]])\n",
      "tensor([[ 1,  2,  3],\n",
      "        [ 4,  5,  6],\n",
      "        [ 7,  8,  9],\n",
      "        [10, 11, 12]])\n",
      "tensor([5, 6])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "b = a[0]\n",
    "c = a[0][1]\n",
    "d = a[1:2]\n",
    "e = a[1:]\n",
    "f = a[:2]\n",
    "g = a[:]\n",
    "h = a[1][1:3]\n",
    "print(b)\n",
    "print(c)\n",
    "print(d)\n",
    "print(e)\n",
    "print(f)\n",
    "print(g)\n",
    "print(h)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### tensor修改\n",
    "和访问类似的方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 3,  2,  1],\n",
      "        [ 4, 11,  6],\n",
      "        [ 9,  8,  7],\n",
      "        [12, 11, 10]])\n"
     ]
    }
   ],
   "source": [
    "a = torch.tensor([[1, 2, 3], [4, 5, 6], [7, 8, 9], [10, 11, 12]])\n",
    "\n",
    "a[0] = torch.tensor([3, 2, 1])\n",
    "a[1][1] = torch.tensor(11)\n",
    "a[2:4] = torch.tensor([[9, 8, 7], [12, 11, 10]])\n",
    "print(a)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
