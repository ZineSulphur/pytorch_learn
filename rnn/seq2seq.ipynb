{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/bentrevett/pytorch-seq2seq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "使用opus-100数据集的en-zh子数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/opus-100\", \"en-zh\")\n",
    "\n",
    "train_data = [x['translation'] for x in ds[\"train\"]]\n",
    "valid_data = [x['translation'] for x in ds[\"validation\"]]\n",
    "test_data = [x['translation'] for x in ds[\"test\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果提示hub连接失败，可是试试换源\n",
    "\n",
    "Huggleface镜像源替换环境变量\n",
    "\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "$env:HF_ENDPOINT = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检验dataset是否下载和加载成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(ds)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "接下来使用spacy进行分词，即将一个句子中的单词和短语分离出来，方便进行相关处理和学习训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分词之前，我们需要下载spacy的相关分析模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download zh_core_web_sm\n",
    "\n",
    "!python -m spacy download en_core_news_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用pip的github连接下载，本地使用pip安装也可，注意安装环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.7.0/zh_core_web_sm-3.7.0-py3-none-any.whl\n",
    "\n",
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "zh_nlp = spacy.load(\"zh_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试加载结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text1 = \"This is amazing!\"\n",
    "test_text2 = \"这好棒啊\"\n",
    "\n",
    "test_token1 = [token.text for token in en_nlp.tokenizer(test_text1)]\n",
    "test_token2 = [token.text for token in zh_nlp.tokenizer(test_text2)]\n",
    "print(test_token1)\n",
    "print(test_token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建一个函数用于tokenizer，将相应的数据集数据进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_en_zh(example, en_nlp, zh_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example[\"en\"])][:max_length]\n",
    "    zh_tokens = [token.text for token in zh_nlp.tokenizer(example[\"zh\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    zh_tokens = [sos_token] + zh_tokens + [eos_token]\n",
    "    return {\"en_tokens\":en_tokens,\"zh_tokens\":zh_tokens}\n",
    "\n",
    "max_length = 1000\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"zh_nlp\": zh_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = ds[\"train\"].map(tokenize_en_zh, fn_kwargs=fn_kwargs)\n",
    "valid_data = ds[\"validation\"].map(tokenize_en_zh, fn_kwargs=fn_kwargs)\n",
    "test_data = ds[\"test\"].map(tokenize_en_zh, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies\n",
    "\n",
    "接下来开始构建词表，将每个单词用一个对应的索引编号来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchtext.vocab\n",
    "\n",
    "min_freq = 1 # 出现次数少于这个的不建立索引\n",
    "# 特殊词元\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "\n",
    "special_tokens = {\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token\n",
    "}\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "zh_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"zh_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    ")\n",
    "\n",
    "# 处理默认返回结果\n",
    "en_vocab.set_default_index(en_vocab[unk_token])\n",
    "zh_vocab.set_default_index(zh_vocab[unk_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看词表建立结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(en_vocab.get_itos()[:10])\n",
    "print(zh_vocab.get_itos()[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建一个对数据集进行numericalize编码的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def numericalize_en_zh(example, en_vocab, zh_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    zh_ids = zh_vocab.lookup_indices(example[\"zh_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"zh_ids\": zh_ids}\n",
    "\n",
    "fn_kwargs = {\"en_vocab\": en_vocab, \"zh_vocab\": zh_vocab}\n",
    "train_data = train_data.map(numericalize_en_zh, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_en_zh, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_en_zh, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看numericalize结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "保存词表en_vocab, zh_vocab和处理好的数据集train_data, valid_data, test_data到文件中备用。（可选）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "folder = \"./data/opus100_en-zh_preprocessed\"\n",
    "\n",
    "def dump_data(folder:str, **kwarg):\n",
    "    if not os.path.exists(folder):\n",
    "        os.makedirs(folder)\n",
    "\n",
    "    for key, value in kwarg.items():\n",
    "        with open(folder + '/' + key + '.json', 'w') as f:\n",
    "            json.dump(value, f)\n",
    "\n",
    "dump_data(folder, en_vocab=en_vocab, zh_vocab=zh_vocab, train_data=train_data, valid_data=valid_data, test_data=test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "读取保存内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'en_vocab': [123], 'zh_vocab': [123], 'train_data': [123], 'valid_data': [123], 'test_data': [123]}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "folder = \"./data/opus100_en-zh_preprocessed/\"\n",
    "files = ['en_vocab.json','zh_vocab.json','train_data.json','valid_data.json','test_data.json']\n",
    "\n",
    "def load_data(folder:str, files:list):\n",
    "    data = {}\n",
    "    for file in files:\n",
    "        with open(folder + '/' + file, 'r') as f:\n",
    "            d = {file.split('.')[0]: json.load(f)}\n",
    "            data.update(d)\n",
    "    return data\n",
    "\n",
    "_data = load_data(folder, files)\n",
    "en_vocab = _data['en_vocab']\n",
    "zh_vocab = _data['zh_vocab']\n",
    "train_data = _data['train_data']\n",
    "valid_data = _data['valid_data']\n",
    "test_data = _data['test_data']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DataLoader"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
