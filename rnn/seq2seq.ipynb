{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reference: https://github.com/bentrevett/pytorch-seq2seq/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 数据准备"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置随机数种子，得到一致的结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "seed = 1234\n",
    "\n",
    "random.seed(seed)\n",
    "np.random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cudnn.deterministic=True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Datasets\n",
    "\n",
    "使用opus-100数据集的en-zh子数据集"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import datasets\n",
    "from datasets import load_dataset\n",
    "\n",
    "ds = load_dataset(\"Helsinki-NLP/opus-100\", \"en-zh\")\n",
    "\n",
    "train_data = ds[\"train\"].shuffle(seed=seed).select(range(100000)) # 采样 x 个数据\n",
    "valid_data = ds[\"validation\"]\n",
    "test_data = ds[\"test\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果提示hub连接失败，可是试试换源\n",
    "\n",
    "Huggleface镜像源替换环境变量\n",
    "\n",
    "export HF_ENDPOINT=https://hf-mirror.com\n",
    "\n",
    "$env:HF_ENDPOINT = \"https://hf-mirror.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检验dataset是否下载和加载成功"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    test: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "    train: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 1000000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['translation'],\n",
      "        num_rows: 2000\n",
      "    })\n",
      "})\n",
      "{'translation': {'en': \"So they couldn't question you.\", 'zh': '为了你不会为了我而撒谎'}}\n"
     ]
    }
   ],
   "source": [
    "print(ds)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tokenizer\n",
    "\n",
    "接下来使用spacy进行分词，即将一个句子中的单词和短语分离出来，方便进行相关处理和学习训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在分词之前，我们需要下载spacy的相关分析模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!python -m spacy download zh_core_web_sm\n",
    "\n",
    "!python -m spacy download en_core_web_sm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "或者使用pip的github连接下载，本地使用pip安装也可，注意安装环境。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "pip install https://github.com/explosion/spacy-models/releases/download/zh_core_web_sm-3.7.0/zh_core_web_sm-3.7.0-py3-none-any.whl\n",
    "\n",
    "pip install https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-3.7.0/en_core_web_sm-3.7.0-py3-none-any.whl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "加载模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "\n",
    "en_nlp = spacy.load(\"en_core_web_sm\")\n",
    "zh_nlp = spacy.load(\"zh_core_web_sm\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试加载结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['This', 'is', 'amazing', '!']\n",
      "['这', '好', '棒', '啊']\n"
     ]
    }
   ],
   "source": [
    "test_text1 = \"This is amazing!\"\n",
    "test_text2 = \"这好棒啊\"\n",
    "\n",
    "test_token1 = [token.text for token in en_nlp.tokenizer(test_text1)]\n",
    "test_token2 = [token.text for token in zh_nlp.tokenizer(test_text2)]\n",
    "print(test_token1)\n",
    "print(test_token2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建一个函数用于tokenizer，将相应的数据集数据进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [01:30<00:00, 1109.30 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:01<00:00, 1025.20 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:02<00:00, 963.36 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def tokenize_en_zh(example, en_nlp, zh_nlp, max_length, lower, sos_token, eos_token):\n",
    "    en_tokens = [token.text for token in en_nlp.tokenizer(example['translation'][\"en\"])][:max_length]\n",
    "    zh_tokens = [token.text for token in zh_nlp.tokenizer(example['translation'][\"zh\"])][:max_length]\n",
    "    if lower:\n",
    "        en_tokens = [token.lower() for token in en_tokens]\n",
    "    en_tokens = [sos_token] + en_tokens + [eos_token]\n",
    "    zh_tokens = [sos_token] + zh_tokens + [eos_token]\n",
    "    return {\"en_tokens\":en_tokens,\"zh_tokens\":zh_tokens}\n",
    "\n",
    "max_length = 100\n",
    "lower = True\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "fn_kwargs = {\n",
    "    \"en_nlp\": en_nlp,\n",
    "    \"zh_nlp\": zh_nlp,\n",
    "    \"max_length\": max_length,\n",
    "    \"lower\": lower,\n",
    "    \"sos_token\": sos_token,\n",
    "    \"eos_token\": eos_token,\n",
    "}\n",
    "\n",
    "train_data = train_data.map(tokenize_en_zh, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(tokenize_en_zh, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(tokenize_en_zh, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试一下分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['translation', 'en_tokens', 'zh_tokens'],\n",
      "    num_rows: 100000\n",
      "})\n",
      "{'translation': {'en': \"So they couldn't question you.\", 'zh': '为了你不会为了我而撒谎'}, 'en_tokens': ['<sos>', 'so', 'they', 'could', \"n't\", 'question', 'you', '.', '<eos>'], 'zh_tokens': ['<sos>', '为了', '你', '不', '会', '为了', '我', '而', '撒谎', '<eos>']}\n"
     ]
    }
   ],
   "source": [
    "print(train_data)\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vocabularies\n",
    "\n",
    "接下来开始构建词表，将每个单词用一个对应的索引编号来表示。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchtext\\vocab\\__init__.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n",
      "e:\\Anaconda\\envs\\pytorch\\Lib\\site-packages\\torchtext\\utils.py:4: UserWarning: \n",
      "/!\\ IMPORTANT WARNING ABOUT TORCHTEXT STATUS /!\\ \n",
      "Torchtext is deprecated and the last released version will be 0.18 (this one). You can silence this warning by calling the following at the beginnign of your scripts: `import torchtext; torchtext.disable_torchtext_deprecation_warning()`\n",
      "  warnings.warn(torchtext._TORCHTEXT_DEPRECATION_MSG)\n"
     ]
    }
   ],
   "source": [
    "import torchtext.vocab\n",
    "\n",
    "min_freq = 2 # 出现次数少于这个的不建立索引\n",
    "# 特殊词元\n",
    "unk_token = \"<unk>\"\n",
    "pad_token = \"<pad>\"\n",
    "sos_token = \"<sos>\"\n",
    "eos_token = \"<eos>\"\n",
    "\n",
    "special_tokens = {\n",
    "    unk_token,\n",
    "    pad_token,\n",
    "    sos_token,\n",
    "    eos_token,\n",
    "}\n",
    "\n",
    "en_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"en_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    # max_tokens=20000, # 限制词表长度，一般不用设置\n",
    ")\n",
    "\n",
    "zh_vocab = torchtext.vocab.build_vocab_from_iterator(\n",
    "    train_data[\"zh_tokens\"],\n",
    "    min_freq=min_freq,\n",
    "    specials=special_tokens,\n",
    "    # max_tokens=60000, # 限制词表长度，一般不用设置\n",
    ")\n",
    "\n",
    "# 处理默认返回结果\n",
    "en_vocab.set_default_index(en_vocab[unk_token])\n",
    "zh_vocab.set_default_index(zh_vocab[unk_token])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看词表建立结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<eos>', '<sos>', '<unk>', '<pad>', 'the', ',', '.', 'of', 'and', 'to']\n",
      "['<eos>', '<sos>', '<unk>', '<pad>', '的', '，', '。', '和', '在', '、']\n",
      "23384\n",
      "29412\n"
     ]
    }
   ],
   "source": [
    "print(en_vocab.get_itos()[:10])\n",
    "print(zh_vocab.get_itos()[:10])\n",
    "print(len(en_vocab))\n",
    "print(len(zh_vocab))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建一个对数据集进行numericalize编码的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 100000/100000 [00:10<00:00, 9417.99 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 7967.01 examples/s]\n",
      "Map: 100%|██████████| 2000/2000 [00:00<00:00, 7632.27 examples/s]\n"
     ]
    }
   ],
   "source": [
    "def numericalize_en_zh(example, en_vocab, zh_vocab):\n",
    "    en_ids = en_vocab.lookup_indices(example[\"en_tokens\"])\n",
    "    zh_ids = zh_vocab.lookup_indices(example[\"zh_tokens\"])\n",
    "    return {\"en_ids\": en_ids, \"zh_ids\": zh_ids}\n",
    "\n",
    "fn_kwargs = {\"en_vocab\": en_vocab, \"zh_vocab\": zh_vocab}\n",
    "train_data = train_data.map(numericalize_en_zh, fn_kwargs=fn_kwargs)\n",
    "valid_data = valid_data.map(numericalize_en_zh, fn_kwargs=fn_kwargs)\n",
    "test_data = test_data.map(numericalize_en_zh, fn_kwargs=fn_kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看numericalize结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'translation': {'en': \"So they couldn't question you.\", 'zh': '为了你不会为了我而撒谎'},\n",
       " 'en_tokens': ['<sos>',\n",
       "  'so',\n",
       "  'they',\n",
       "  'could',\n",
       "  \"n't\",\n",
       "  'question',\n",
       "  'you',\n",
       "  '.',\n",
       "  '<eos>'],\n",
       " 'zh_tokens': ['<sos>', '为了', '你', '不', '会', '为了', '我', '而', '撒谎', '<eos>'],\n",
       " 'en_ids': [1, 82, 61, 150, 46, 442, 16, 6, 0],\n",
       " 'zh_ids': [1, 354, 14, 16, 60, 354, 10, 89, 5518, 0]}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将ids使用with_format转换为pytorch的tensor类型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_type = \"torch\"\n",
    "format_columns = [\"en_ids\",\"zh_ids\"]\n",
    "\n",
    "train_data = train_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "valid_data = valid_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")\n",
    "\n",
    "test_data = test_data.with_format(\n",
    "    type=data_type, columns=format_columns, output_all_columns=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "检查结果"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "{'en_ids': tensor([  1,  82,  61, 150,  46, 442,  16,   6,   0]), 'zh_ids': tensor([   1,  354,   14,   16,   60,  354,   10,   89, 5518,    0]), 'translation': {'en': \"So they couldn't question you.\", 'zh': '为了你不会为了我而撒谎'}, 'en_tokens': ['<sos>', 'so', 'they', 'could', \"n't\", 'question', 'you', '.', '<eos>'], 'zh_tokens': ['<sos>', '为了', '你', '不', '会', '为了', '我', '而', '撒谎', '<eos>']}\n"
     ]
    }
   ],
   "source": [
    "print(type(train_data[0][\"en_ids\"]))\n",
    "print(train_data[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "最后一步将数据装入pytorch的DataLoader中"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "collate_fn 接收一个batch将其中的en_ids和zh_ids进行padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "def get_collate_fn(pad_index):\n",
    "    def collate_fn(batch):\n",
    "        batch_en_ids = [example[\"en_ids\"] for example in batch]\n",
    "        batch_zh_ids = [example[\"zh_ids\"] for example in batch]\n",
    "        batch_en_ids = nn.utils.rnn.pad_sequence(batch_en_ids, padding_value=pad_index)\n",
    "        batch_zh_ids = nn.utils.rnn.pad_sequence(batch_zh_ids, padding_value=pad_index)\n",
    "        batch = {\n",
    "            \"en_ids\": batch_en_ids,\n",
    "            \"zh_ids\": batch_zh_ids,\n",
    "        }\n",
    "        return batch\n",
    "\n",
    "    return collate_fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来创建dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.utils.data\n",
    "\n",
    "def get_data_loader(dataset, batch_size, pad_index, shuffle=False):\n",
    "    collate_fn = get_collate_fn(pad_index)\n",
    "    data_loader = torch.utils.data.DataLoader(\n",
    "        dataset=dataset,\n",
    "        batch_size=batch_size,\n",
    "        collate_fn=collate_fn,\n",
    "        shuffle=shuffle\n",
    "    )\n",
    "    return data_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "pad_index = en_vocab[pad_token]\n",
    "\n",
    "train_data_loader = get_data_loader(train_data, batch_size, pad_index, shuffle=True)\n",
    "valid_data_loader = get_data_loader(valid_data, batch_size, pad_index)\n",
    "test_data_loader = get_data_loader(test_data, batch_size, pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 创建模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x = [length, batch]\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded = [length, batch, embedding dim]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "        # outputs = [length, batch, hidden_dim * n directions] # n directions 单向LSTM为1双向为2\n",
    "        # hidden,cell = [n layers * n directions, batch, hidden dim]\n",
    "        return hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, output_dim, embedding_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.output_dim=output_dim\n",
    "        self.hidden_dim=hidden_dim\n",
    "        self.n_layers=n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.fc_out = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, hidden, cell):\n",
    "        # x = [batch]\n",
    "        # hidden,cell = [n layers * n directions, batch, hidden dim]\n",
    "        x = x.unsqueeze(0)\n",
    "        # x = [1, batch]\n",
    "        embedded = self.dropout(self.embedding(x))\n",
    "        # embedded = [1, batch, embedding dim]\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # output = [seq length, batch, hidden dim * n directions]\n",
    "        # hidden/cell = [n layers * n directions, batch, hidden dim]\n",
    "        # seq length & n directions = 1 here\n",
    "        prediction = self.fc_out(output.squeeze(0))\n",
    "        # prediction = [batch, output dim]\n",
    "        return prediction, hidden, cell"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Seq2Seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "import random\n",
    "\n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder, device):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.device = device\n",
    "        assert (\n",
    "            encoder.hidden_dim == decoder.hidden_dim\n",
    "            ), \"Hidden dimensions of encoder and decoder must be equal!\"\n",
    "        assert (\n",
    "            encoder.n_layers == decoder.n_layers\n",
    "            ), \"Encoder and decoder must have equal number of layers!\"\n",
    "        \n",
    "    def forward(self, src, trg, teacher_forcing_ratio):\n",
    "        # src = [src length, batch]\n",
    "        # trg = [trg length, batch]\n",
    "        # if teacher_forcing_ratio is 0.75 we user ground-truth inputs 75% of the time\n",
    "        batch_size = trg.shape[1]\n",
    "        trg_length = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "        # tensor to store decoder outputs\n",
    "        outputs = torch.zeros(trg_length, batch_size, trg_vocab_size).to(self.device)\n",
    "        # last hidden state of encoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "        # first input to decoder is <sos>\n",
    "        input = trg[0, :]\n",
    "        for t in range(1, trg_length):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            # place predictions in a tensor\n",
    "            outputs[t] = output\n",
    "            # decide if we are going to use teacher forcing or not\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            # get higgest predicted token from predictions\n",
    "            top1 = output.argmax(1)\n",
    "            # if teacher_force use actual as input else top1\n",
    "            input = trg[t] if teacher_force else top1\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 初始化模型\n",
    "\n",
    "训练之前需要先初始化模型和加载数据集（已完成）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = len(en_vocab)\n",
    "output_dim = len(zh_vocab)\n",
    "encoder_embedding_dim = 256\n",
    "decoder_embedding_dim = 256\n",
    "hidden_dim = 512\n",
    "n_layers = 2\n",
    "encoder_dropout = 0.5\n",
    "decoder_dropout = 0.5\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "encoder = Encoder(\n",
    "    input_dim,\n",
    "    encoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    encoder_dropout\n",
    ")\n",
    "\n",
    "decoder = Decoder(\n",
    "    output_dim,\n",
    "    decoder_embedding_dim,\n",
    "    hidden_dim,\n",
    "    n_layers,\n",
    "    decoder_dropout\n",
    ")\n",
    "\n",
    "model = Seq2Seq(encoder, decoder, device).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "均匀分布初始化权重\n",
    "\n",
    "使用apply的时候，这个函数将会在每个模块和子模块中调用，对每个模块使用nn.init.uniform_进行均匀采样"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Seq2Seq(\n",
       "  (encoder): Encoder(\n",
       "    (embedding): Embedding(23384, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       "  (decoder): Decoder(\n",
       "    (embedding): Embedding(29412, 256)\n",
       "    (rnn): LSTM(256, 512, num_layers=2, dropout=0.5)\n",
       "    (fc_out): Linear(in_features=512, out_features=29412, bias=True)\n",
       "    (dropout): Dropout(p=0.5, inplace=False)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def init_weights(m):\n",
    "    for name, param in m.named_parameters():\n",
    "        nn.init.uniform_(param.data, -0.08, 0.08)\n",
    "\n",
    "model.apply(init_weights)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看模型参数个数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The mode has 35,960,548 trainable parameters\n"
     ]
    }
   ],
   "source": [
    "def count_parameters(model):\n",
    "    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"The mode has {count_parameters(model):,} trainable parameters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 优化器"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 损失函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "训练用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_fn(model, data_loader, optimizer, criterion, clip, teacher_forcing_ratio, device):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    for i, batch in enumerate(data_loader):\n",
    "        src = batch[\"en_ids\"].to(device)\n",
    "        trg = batch[\"zh_ids\"].to(device)\n",
    "        # src/trg = [length, batch]\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, trg, teacher_forcing_ratio)\n",
    "        # output = [length, batch, vocab size]\n",
    "        output_dim = output.shape[-1]\n",
    "        output = output[1:].view(-1, output_dim)\n",
    "        # output = [(length-1)*batch, vocab size]\n",
    "        trg = trg[1:].view(-1)\n",
    "        # trg = [(length-1)*batch]\n",
    "        loss = criterion(output, trg)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "evaluate用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_fn(model, data_loader, criterion, device):\n",
    "    model.eval()\n",
    "    epoch_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(data_loader):\n",
    "            src = batch[\"en_ids\"].to(device)\n",
    "            trg = batch[\"zh_ids\"].to(device)\n",
    "            output = model(src, trg, 0) # turnoff teacher forcing\n",
    "            output_dim = output.shape[-1]\n",
    "            output = output[1:].view(-1, output_dim)\n",
    "            trg = trg[1:].view(-1)\n",
    "            loss = criterion(output, trg)\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss/len(data_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "开始训练"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  5%|▌         | 1/20 [24:00<7:36:07, 1440.38s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   6.667 | Train PPL: 785.712\n",
      "\tValid Loss:   6.594 | Valid PPL: 730.956\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|█         | 2/20 [47:59<7:11:58, 1439.91s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   6.056 | Train PPL: 426.514\n",
      "\tValid Loss:   6.443 | Valid PPL: 628.249\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▌        | 3/20 [1:11:56<6:47:31, 1438.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.760 | Train PPL: 317.385\n",
      "\tValid Loss:   6.391 | Valid PPL: 596.191\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 20%|██        | 4/20 [1:35:57<6:23:48, 1439.30s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.554 | Train PPL: 258.144\n",
      "\tValid Loss:   6.273 | Valid PPL: 529.931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 25%|██▌       | 5/20 [2:00:29<6:02:46, 1451.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.392 | Train PPL: 219.718\n",
      "\tValid Loss:   6.210 | Valid PPL: 497.898\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 30%|███       | 6/20 [2:25:10<5:40:59, 1461.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.241 | Train PPL: 188.834\n",
      "\tValid Loss:   6.155 | Valid PPL: 471.056\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 35%|███▌      | 7/20 [2:49:27<5:16:18, 1459.85s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.116 | Train PPL: 166.673\n",
      "\tValid Loss:   6.103 | Valid PPL: 447.229\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 40%|████      | 8/20 [3:15:35<4:58:52, 1494.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   5.000 | Train PPL: 148.407\n",
      "\tValid Loss:   6.084 | Valid PPL: 438.979\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 45%|████▌     | 9/20 [3:41:22<4:36:58, 1510.79s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.896 | Train PPL: 133.725\n",
      "\tValid Loss:   6.087 | Valid PPL: 439.964\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 50%|█████     | 10/20 [4:06:59<4:13:09, 1518.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.810 | Train PPL: 122.781\n",
      "\tValid Loss:   6.082 | Valid PPL: 438.076\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 55%|█████▌    | 11/20 [4:31:19<3:45:06, 1500.72s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.726 | Train PPL: 112.858\n",
      "\tValid Loss:   6.026 | Valid PPL: 414.110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 60%|██████    | 12/20 [4:54:32<3:15:43, 1467.93s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.648 | Train PPL: 104.374\n",
      "\tValid Loss:   6.037 | Valid PPL: 418.483\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 65%|██████▌   | 13/20 [5:17:54<2:48:57, 1448.20s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.568 | Train PPL:  96.335\n",
      "\tValid Loss:   6.081 | Valid PPL: 437.270\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 70%|███████   | 14/20 [5:41:23<2:23:38, 1436.34s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.506 | Train PPL:  90.532\n",
      "\tValid Loss:   6.066 | Valid PPL: 430.855\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 75%|███████▌  | 15/20 [6:04:45<1:58:48, 1425.78s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.440 | Train PPL:  84.774\n",
      "\tValid Loss:   6.038 | Valid PPL: 419.192\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 80%|████████  | 16/20 [6:29:24<1:36:07, 1441.88s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.384 | Train PPL:  80.181\n",
      "\tValid Loss:   6.084 | Valid PPL: 438.971\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 85%|████████▌ | 17/20 [6:53:14<1:11:55, 1438.39s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.329 | Train PPL:  75.862\n",
      "\tValid Loss:   6.106 | Valid PPL: 448.764\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 90%|█████████ | 18/20 [7:16:45<47:40, 1430.20s/it]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.269 | Train PPL:  71.452\n",
      "\tValid Loss:   6.136 | Valid PPL: 462.366\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 95%|█████████▌| 19/20 [7:40:18<23:44, 1424.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.228 | Train PPL:  68.547\n",
      "\tValid Loss:   6.143 | Valid PPL: 465.648\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 20/20 [8:03:37<00:00, 1450.87s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTrain Loss:   4.176 | Train PPL:  65.131\n",
      "\tValid Loss:   6.143 | Valid PPL: 465.423\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import tqdm\n",
    "\n",
    "n_epochs = 20\n",
    "clip = 1.0\n",
    "teacher_forcing_ratio = 0.5\n",
    "\n",
    "best_valid_loss = float(\"inf\")\n",
    "\n",
    "for epoch in tqdm.tqdm(range(n_epochs)):\n",
    "    train_loss = train_fn(\n",
    "        model,\n",
    "        train_data_loader,\n",
    "        optimizer,\n",
    "        criterion,\n",
    "        clip,\n",
    "        teacher_forcing_ratio,\n",
    "        device\n",
    "    )\n",
    "    \n",
    "    valid_loss = evaluate_fn(\n",
    "        model,\n",
    "        valid_data_loader,\n",
    "        criterion,\n",
    "        device\n",
    "    )\n",
    "    if valid_loss < best_valid_loss:\n",
    "        best_valid_loss = valid_loss\n",
    "        torch.save(model.state_dict(), \"./model/s2s-enzh-model.pt\")\n",
    "    print(f\"\\tTrain Loss: {train_loss:7.3f} | Train PPL: {np.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\tValid Loss: {valid_loss:7.3f} | Valid PPL: {np.exp(valid_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 评估模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先测试模型的loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\tTest Loss:   5.983 | Test PPL: 396.545\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"./model/s2s-enzh-model.pt\"))\n",
    "test_loss = evaluate_fn(model, test_data_loader, criterion, device)\n",
    "print(f\"\\tTest Loss: {test_loss:7.3f} | Test PPL: {np.exp(test_loss):7.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来评估模型的BELU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "首先是翻译用函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate_sentence(sentence, model, en_nlp, zh_nlp, en_vocab, zh_vocab, \n",
    "                       lower, sos_token, eos_token, device, max_output_length=25,):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        if isinstance(sentence, str):\n",
    "            tokens = [token.text for token in en_nlp.tokenizer(sentence)]\n",
    "        else:\n",
    "            tokens = [token for token in sentence]\n",
    "        tokens = [sos_token] + tokens + [eos_token]\n",
    "        ids = en_vocab.lookup_indices(tokens)\n",
    "        tensor = torch.LongTensor(ids).unsqueeze(-1).to(device)\n",
    "        hidden, cell = model.encoder(tensor)\n",
    "        inputs = zh_vocab.lookup_indices([sos_token])\n",
    "        for _ in range(max_output_length):\n",
    "            inputs_tensor = torch.LongTensor([inputs[-1]]).to(device)\n",
    "            output, hidden, cell = model.decoder(inputs_tensor, hidden, cell)\n",
    "            predicted_token = output.argmax(-1).item()\n",
    "            inputs.append(predicted_token)\n",
    "            if predicted_token == zh_vocab[eos_token]:\n",
    "                break\n",
    "        tokens = zh_vocab.lookup_tokens(inputs)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试翻译函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The Global Programme of Action Coordination Office, with the financial support of Belgium, is currently assisting Egypt, Nigeria, United Republic of Tanzania, Sri Lanka and Yemen to develop pilot national programmes of action for the protection of the marine environment from land-based activities.\n",
      "9. 《全球行动纲领》协调处得到比利时的财政支持，目前正帮助埃及、尼日利亚、坦桑尼亚联合共和国、斯里兰卡和也门制订试行的保护海洋环境免受陆地活动影响的国家行动方案。\n",
      "['<sos>', '<unk>', '.', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>', '<unk>']\n"
     ]
    }
   ],
   "source": [
    "sentence = test_data[0]['translation']['en']\n",
    "expected_translation = test_data[0]['translation']['zh']\n",
    "\n",
    "print(sentence)\n",
    "print(expected_translation)\n",
    "\n",
    "translation = translate_sentence(sentence, model, en_nlp, zh_nlp, en_vocab, zh_vocab,\n",
    "                                 lower, sos_token, eos_token, device)\n",
    "print(translation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来将test_data进行翻译"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:26<00:00, 75.52it/s]\n"
     ]
    }
   ],
   "source": [
    "translations = [translate_sentence(example['translation'][\"en\"], model, en_nlp, zh_nlp, en_vocab, zh_vocab,\n",
    "                                 lower, sos_token, eos_token, device)\n",
    "                for example in tqdm.tqdm(test_data)]\n",
    "\n",
    "predictions = [\"\".join(translation[1:-1]) for translation in translations]\n",
    "references = [example['translation'][\"zh\"] for example in test_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看预测和参考内容"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<unk>.<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk>\n",
      "9. 《全球行动纲领》协调处得到比利时的财政支持，目前正帮助埃及、尼日利亚、坦桑尼亚联合共和国、斯里兰卡和也门制订试行的保护海洋环境免受陆地活动影响的国家行动方案。\n"
     ]
    }
   ],
   "source": [
    "print(predictions[0])\n",
    "print(references[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "将结果tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenizer_fn(nlp, lower):\n",
    "    def tokenizer_fn(s):\n",
    "        tokens = [token.text for token in nlp.tokenizer(s)]\n",
    "        if lower:\n",
    "            tokens = [token.lower() for token in tokens]\n",
    "        return tokens\n",
    "    return tokenizer_fn\n",
    "\n",
    "tokenizer_fn = get_tokenizer_fn(zh_nlp, lower)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "测试函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['<un', 'k>', '.', '<unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><unk><un', 'k>']\n",
      "['9', '.', '《', '全球', '行动', '纲领', '》', '协调', '处', '得到', '比利时', '的', '财政', '支持', '，', '目前', '正', '帮助', '埃及', '、', '尼日利亚', '、', '坦桑尼亚', '联合', '共和国', '、', '斯里兰卡', '和', '也门', '制订', '试行', '的', '保护', '海洋', '环境', '免受', '陆地', '活动', '影响', '的', '国家', '行动', '方案', '。']\n"
     ]
    }
   ],
   "source": [
    "print(tokenizer_fn(predictions[0]))\n",
    "print(tokenizer_fn(references[0]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "计算bleu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "results = bleu.compute(\n",
    "    predictions=predictions, references=references, tokenizer=tokenizer_fn\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "查看结果BLEU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.002740053664609093, 'precisions': [0.2146313838550247, 0.013195639701663799, 0.0032295569047926624, 0.0012392477037469019], 'brevity_penalty': 0.26555402230057806, 'length_ratio': 0.4299342614931716, 'translation_length': 19424, 'reference_length': 45179}\n"
     ]
    }
   ],
   "source": [
    "print(results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
