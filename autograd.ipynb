{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### [torch.autograd](https://pytorch.org/docs/stable/autograd.html#)\n",
    "autograd主要提供自动微分的类和函数，需要在创建tensor时指定requires_grad=True，并且只支持float相关类型和复数tensor类型。\n",
    "\n",
    "在实际中主要用于梯度下降和反向传播的内容，主要包括backward和grad等方法。\n",
    "|||\n",
    "|---|---|\n",
    "|backward|用于启动反向传播，计算图中所有叶子节点的梯度|\n",
    "|grad|计算输出相对输入的梯度|\n",
    "\n",
    "***\n",
    "```python\n",
    "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False)\n",
    "```\n",
    "**Parameters**\n",
    "* outputs   - 输出张量\n",
    "* inputs    - 输入张量\n",
    "* grad_outputs  - 如果outputs不是标量，需要使用此参数\n",
    "* retain_graoh  - 保留计算图\n",
    "* create_graph  - 创建计算图，用于高阶导数计算\n",
    "* allow_unused  - 允许输入变量不进入计算\n",
    "* is_grads_batched  - 如果为True，grad_output中每个张量的第一个维度将被解释为批量维度\n",
    "* materialize_grads - 如果为True，输入的张量使用0进行填充\n",
    "**Return Type**\n",
    "* Tuple[Tensor,] - 输出梯度元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]),)\n",
      "(tensor([[1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.]]),)\n",
      "(tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "x = torch.rand(3, 4)\n",
    "x.requires_grad_()\n",
    "\n",
    "# y为x中元素和，为标量\n",
    "y = torch.sum(x)\n",
    "grad1 = autograd.grad(outputs=y, inputs=x)\n",
    "print(grad1)\n",
    "\n",
    "# y为向量\n",
    "y = x[:,0] + x[:,1]\n",
    "grad2 = autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(grad2)\n",
    "\n",
    "# 二阶导数\n",
    "y = x ** 2\n",
    "grad3 = autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y), create_graph=True)\n",
    "grad4 = autograd.grad(outputs=grad3, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(grad4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "```python\n",
    "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)\n",
    "```\n",
    "**Parameters**\n",
    "* tensors       - 指定要求梯度的张量\n",
    "* grad_tensors  - 通常是对应张量的每个元素的梯度\n",
    "* retain_graph  - 保留计算图\n",
    "* create_graph  - 创建计算图，用于高阶导数计算\n",
    "* inputs        - 指定计算的张量，若不指定则为所有叶子张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., grad_fn=<AddBackward0>) tensor(2.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = x ** 2 + y\n",
    "z.backward()\n",
    "print(z, x.grad, y.grad)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
