{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### [torch.autograd](https://pytorch.org/docs/stable/autograd.html#)\n",
    "autograd主要提供自动微分的类和函数，需要在创建tensor时指定requires_grad=True，并且只支持float相关类型和复数tensor类型。\n",
    "\n",
    "PyTorch提供backward()和torch.autograd.grad()两种求梯度的方法。backward()会将梯度填充到叶子节点的.grad字段，而torch.autograd.grad()直接返回梯度结果。\n",
    "|||\n",
    "|---|---|\n",
    "|backward|用于启动反向传播，计算图中所有叶子节点的梯度|\n",
    "|grad|计算输出相对输入的梯度|\n",
    "\n",
    "***\n",
    "```python\n",
    "torch.autograd.grad(outputs, inputs, grad_outputs=None, retain_graph=None, create_graph=False, only_inputs=True, allow_unused=None, is_grads_batched=False, materialize_grads=False)\n",
    "```\n",
    "**Parameters**\n",
    "* outputs   - 输出张量\n",
    "* inputs    - 输入张量\n",
    "* grad_outputs  - 如果outputs不是标量，需要使用此参数\n",
    "* retain_graoh  - 保留计算图\n",
    "* create_graph  - 创建计算图，用于高阶导数计算\n",
    "* allow_unused  - 允许输入变量不进入计算\n",
    "* is_grads_batched  - 如果为True，grad_output中每个张量的第一个维度将被解释为批量维度\n",
    "* materialize_grads - 如果为True，输入的张量使用0进行填充\n",
    "**Return Type**\n",
    "* Tuple[Tensor,] - 输出梯度元组"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(tensor([[1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.],\n",
      "        [1., 1., 1., 1.]]),)\n",
      "(tensor([[1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.],\n",
      "        [1., 1., 0., 0.]]),)\n",
      "(tensor([[2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.],\n",
      "        [2., 2., 2., 2.]]),)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "x = torch.rand(3, 4)\n",
    "x.requires_grad_()\n",
    "\n",
    "# y为x中元素和，为标量\n",
    "y = torch.sum(x)\n",
    "grad1 = autograd.grad(outputs=y, inputs=x)\n",
    "print(grad1)\n",
    "\n",
    "# y为向量\n",
    "y = x[:,0] + x[:,1]\n",
    "grad2 = autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(grad2)\n",
    "\n",
    "# 二阶导数\n",
    "y = x ** 2\n",
    "grad3 = autograd.grad(outputs=y, inputs=x, grad_outputs=torch.ones_like(y), create_graph=True)\n",
    "grad4 = autograd.grad(outputs=grad3, inputs=x, grad_outputs=torch.ones_like(y))\n",
    "print(grad4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "```python\n",
    "torch.autograd.backward(tensors, grad_tensors=None, retain_graph=None, create_graph=False, grad_variables=None, inputs=None)\n",
    "```\n",
    "**Parameters**\n",
    "* tensors       - 指定要求梯度的张量\n",
    "* grad_tensors  - 通常是对应张量的每个元素的梯度\n",
    "* retain_graph  - 保留计算图\n",
    "* create_graph  - 创建计算图，用于高阶导数计算\n",
    "* inputs        - 指定计算的张量，若不指定则为所有叶子张量"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(3., grad_fn=<AddBackward0>) tensor(2.) tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import autograd\n",
    "\n",
    "x = torch.tensor(1.0, requires_grad=True)\n",
    "y = torch.tensor(2.0, requires_grad=True)\n",
    "z = x ** 2 + y\n",
    "z.backward()\n",
    "print(z, x.grad, y.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### [torch.autograd.Function](https://pytorch.org/docs/stable/autograd.html#function)\n",
    "包含了一系列用于定义自定义操作的函数，这些操作可以在反向传播时自动计算梯度。Function中的操作类似于计算图中的边。\n",
    "|||\n",
    "|---|---|\n",
    "|Function.forward|自定义前向传播逻辑|\n",
    "|Function.backward|自定义反向传播逻辑|\n",
    "|Function.jvp|自定义jvp过程|\n",
    "|Function.vmap|自定义torch.vmap()的求导过程|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(7.3891, grad_fn=<ExpBackward>)\n",
      "tensor(7.3891)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# ex的自动求导\n",
    "class Exp(torch.autograd.Function):\n",
    "    @staticmethod\n",
    "    def forward(ctx, i):\n",
    "        result = i.exp()\n",
    "        ctx.save_for_backward(result)\n",
    "        return result\n",
    "    \n",
    "    @staticmethod\n",
    "    def backward(ctx, grad_output):\n",
    "        result, = ctx.saved_tensors\n",
    "        return grad_output * result\n",
    "    \n",
    "input = torch.tensor(2.0, requires_grad=True)\n",
    "output = Exp.apply(input)\n",
    "print(output)\n",
    "print(input.grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### [torch.autograd.functional](https://pytorch.org/docs/stable/autograd.html#functional-higher-level-api)\n",
    "torch.autograd.functional这个为autograd提供了更高级api，并且可以计算jacobians核hessians等。\n",
    "|||\n",
    "|---|---|\n",
    "|functional.jacobian|计算给定函数的雅可比|\n",
    "|functional.hessian|计算给定函数的黑森|\n",
    "|functional.vjp|计算给定点的向量和雅可比的点积|\n",
    "|functional.jvp|计算给定点的雅可比和向量的点积|\n",
    "|functional.vhp|计算给定点的向量和黑森的点积|\n",
    "|functional.hvp|计算给定点的黑森和向量的点积|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***\n",
    "### 其它\n",
    "* torch.autograd.gradcheck 用于检查数值梯度与自动微分得到的梯度是否一致，这是确保正确性的一个有用工具。\n",
    "* torch.autograd.detect_anomaly 在自动求导时检测错误产生路径，有助于调试。\n",
    "* torch.autograd.grad_mode 允许用户设置是否需要梯度，例如在模型评估时通常不需要计算梯度。"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
